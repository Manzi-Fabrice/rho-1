{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bacdcd-21d8-42e3-bb32-3e0763c9f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "dataset = load_dataset(\"cerebras/SlimPajama-627B\", split=\"train\", streaming=True)\n",
    "target_samples = list(dataset.take(20_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d12e3ab-09a4-4aeb-92a9-5e28471d986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving the dataset to disk for east retrival \n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(target_samples)\n",
    "dataset.save_to_disk(\"tiny_slimpajama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee6fe5e-12d7-44bc-a967-57b576a96268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the disk\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"tiny_slimpajama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3014f3-b360-4b62-a214-7b74f8a0339e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the dataset \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel \n",
    "import torch \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"reference_model\")\n",
    "reference_model = GPT2LMHeadModel.from_pretrained(\"reference_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "reference_model.to(device)\n",
    "reference_model.eval() \n",
    "\n",
    "data = tokenizer(\n",
    "    dataset[\"text\"], \n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    max_length = 128,\n",
    "    return_tensors = \"pt\",\n",
    "    return_attention_mask = True,\n",
    ")\n",
    "torch.save(data, \"tokens_target_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ea9f84-2fc6-4e80-93a7-99e2d46f974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Tokens For Testing \n",
    "# tokens = tokenizer.convert_ids_to_tokens(data[\"input_ids\"][0])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd5cdc4-7e55-420c-892c-3723973ec5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "import torch.serialization\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "torch.serialization.add_safe_globals([BatchEncoding])\n",
    "data = torch.load(\"tokens_target_model.pt\")\n",
    "big_dataset = TensorDataset(data[\"input_ids\"], data[\"attention_mask\"])\n",
    "dataloader_big_dataset = DataLoader(big_dataset, batch_size = 4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3308943d-18e9-43d8-afe3-23d6e1c335f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the reference model to calculate the LRM for the new dataset \n",
    "\n",
    "import torch.nn.functional as F \n",
    "reference_losses = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i, batch in enumerate(dataloader_big_dataset):\n",
    "    input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "\n",
    "    with torch.no_grad(): # not track the gradients and update weights \n",
    "        outputs = reference_model(input_ids = input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = outputs.logits # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        cut_logits = logits[:, :-1, :]\n",
    "        cut_labels = input_ids[:, 1:]\n",
    "        cut_attention = attention_mask[:, 1:]\n",
    "\n",
    "        probs = F.softmax(cut_logits, dim=-1)\n",
    "        true_token_probs = probs.gather(2, cut_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        reference_loss = -torch.log(true_token_probs + 1e-9) # We add small value to avoild log(0)\n",
    "        reference_loss = reference_loss* cut_attention \n",
    "        reference_losses.append(reference_loss)\n",
    "\n",
    "torch.save(torch.cat(reference_losses, dim=0), \"reference_loss_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81d72caf-ceb0-4d7e-a5f5-dfd4a617a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Target Model \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel \n",
    "from torch.optim import AdamW\n",
    "\n",
    "target_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "target_model.resize_token_embeddings(len(tokenizer))\n",
    "target_model.to(device)\n",
    "target_model.eval() \n",
    "optimizer = AdamW(target_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1d342e-44c0-442b-8caa-b53bc313948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_losses = []\n",
    "\n",
    "for batch in dataloader_big_dataset:\n",
    "    input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        outputs = target_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        shift_attention = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "        probs = F.softmax(shift_logits, dim=-1)\n",
    "        true_token_probs = probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        target_loss = -torch.log(true_token_probs + 1e-9) * shift_attention\n",
    "\n",
    "        target_losses.append(target_loss)\n",
    "\n",
    "torch.save(torch.cat(reference_losses, dim=0), \"target_loss_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dcc16bd-d7e2-47da-bf7a-d6ef644d8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " slm loss over top 40% tokens is 3.7647\n"
     ]
    }
   ],
   "source": [
    "ref_los = torch.load(\"reference_loss_final.pt\")   \n",
    "target_los = torch.load(\"target_loss_final.pt\")  \n",
    "\n",
    "excess_loss = target_los - ref_los  \n",
    "\n",
    "k = 0.4  # top 40%\n",
    "flat_excess = excess_loss[ref_los > 0].flatten()  \n",
    "threshold = torch.quantile(flat_excess, 1 - k)\n",
    "\n",
    "mask = (excess_loss >= threshold) & (ref_los > 0)  # top-k mask\n",
    "\n",
    "pad = torch.zeros(mask.size(0), 1).bool() # Pad BOS token as the first token doesn't count\n",
    "topk_mask_full = torch.cat([pad, mask], dim=1) \n",
    "\n",
    "# Checking evaluate slm loss\n",
    "masked_loss = target_los * mask.float()\n",
    "slm_loss = masked_loss.sum() / mask.float().sum()\n",
    "print(f\" slm loss over top {int(k*100)}% tokens is {slm_loss:.4f}\")\n",
    "torch.save(topk_mask_full.float(), \"topk_mask.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b010733-8dc7-442f-a6c6-dea2fc8a96de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('slm_target_model/tokenizer_config.json',\n",
       " 'slm_target_model/special_tokens_map.json',\n",
       " 'slm_target_model/vocab.json',\n",
       " 'slm_target_model/merges.txt',\n",
       " 'slm_target_model/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the target model on the hard token \n",
    "\n",
    "topk_mask_full = torch.load(\"topk_mask.pt\").float()\n",
    "start_idx = 0 \n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch_idx, batch in enumerate(dataloader_big_dataset):\n",
    "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        mask_batch = topk_mask_full[start_idx:start_idx + batch_size, :].to(device)\n",
    "        start_idx += batch_size\n",
    "\n",
    "        outputs = target_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        shift_logits     = logits[:, :-1, :]\n",
    "        shift_labels     = input_ids[:, 1:]\n",
    "        shift_mask       = mask_batch[:, :-1]\n",
    "        shift_attention  = attention_mask[:, 1:]\n",
    "\n",
    "\n",
    "        probs = F.softmax(shift_logits, dim=-1)\n",
    "        true_token_probs = probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        token_loss = -torch.log(true_token_probs + 1e-9)\n",
    "\n",
    "        selected_loss = token_loss * shift_mask\n",
    "        slm_loss = selected_loss.sum() / shift_mask.sum()\n",
    "\n",
    "        slm_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "target_model.save_pretrained(\"slm_target_model\")\n",
    "tokenizer.save_pretrained(\"slm_target_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848ab69-60ef-47ff-b5b7-d650eb20649f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
